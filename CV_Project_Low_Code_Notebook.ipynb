{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9VXT5unCaBd"
   },
   "source": [
    "# Introduction to Computer Vision: Plant Seedlings Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Fgd2jg4f0XH"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upANL8_-f2gy"
   },
   "source": "### Contextet"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCxSmokWEKUJ"
   },
   "source": [
    " Testing Making changes to github\n",
    " \n",
    " In recent times, the field of agriculture has been in urgent need of modernizing, since the amount of manual work people need to put in to check if plants are growing correctly is still highly extensive. Despite several advances in agricultural technology, people working in the agricultural industry still need to have the ability to sort and recognize different plants and weeds, which takes a lot of time and effort in the long term. The potential is ripe for this trillion-dollar industry to be greatly impacted by technological innovations that cut down on the requirement for manual labor, and this is where Artificial Intelligence can actually benefit the workers in this field, as **the time and energy required to identify plant seedlings will be greatly shortened by the use of AI and Deep Learning.** The ability to do so far more efficiently and even more effectively than experienced manual labor, could lead to better crop yields, the freeing up of human inolvement for higher-order agricultural decision making, and in the long term will result in more sustainable environmental practices in agriculture as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpUtbBsMf_9O"
   },
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGW0M77cgDtf"
   },
   "source": [
    "The aim of this project is to Build a Convolutional Neural Netowrk to classify plant seedlings into their respective categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPrWNbdYgMQz"
   },
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI2Xb-WqgQSA"
   },
   "source": [
    "The Aarhus University Signal Processing group, in collaboration with the University of Southern Denmark, has recently released a dataset containing **images of unique plants belonging to 12 different species.**\n",
    "\n",
    "- The dataset can be download from Olympus.\n",
    "- The data file names are:\n",
    "    - images.npy\n",
    "    - Labels.csv\n",
    "- Due to the large volume of data, the images were converted to the images.npy file and the labels are also put into Labels.csv, so that you can work on the data/project seamlessly without having to worry about the high data volume.\n",
    "\n",
    "- The goal of the project is to create a classifier capable of determining a plant's species from an image.\n",
    "\n",
    "**List of Species**\n",
    "\n",
    "- Black-grass\n",
    "- Charlock\n",
    "- Cleavers\n",
    "- Common Chickweed\n",
    "- Common Wheat\n",
    "- Fat Hen\n",
    "- Loose Silky-bent\n",
    "- Maize\n",
    "- Scentless Mayweed\n",
    "- Shepherds Purse\n",
    "- Small-flowered Cranesbill\n",
    "- Sugar beet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upC6c-WCVTZx"
   },
   "source": [
    "### **Note: Please use GPU runtime on Google Colab to execute the code faster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYjByoe294Jz"
   },
   "source": [
    "## **Please read the instructions carefully before starting the project.**\n",
    "\n",
    "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
    "\n",
    "* Blanks '_______' are provided in the notebook that need to be filled with an appropriate code to get the correct result\n",
    "\n",
    "* With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space\n",
    "\n",
    "* Identify the task to be performed correctly and only then proceed to write the required code\n",
    "\n",
    "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\"\n",
    "\n",
    "* Running incomplete code may throw an error\n",
    "\n",
    "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors\n",
    "\n",
    "* Add the results/observations derived from the analysis in the presentation and submit the same in .pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqFzmTb0BKKW"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ix03xE9wbuDQ"
   },
   "outputs": [],
   "source": [
    "# Installing the libraries with the specified version.\n",
    "# uncomment and run the following line if Google Colab is being used\n",
    "# !pip install tensorflow==2.15.0 scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 opencv-python==4.8.0.76 -q --user"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-ZZQtLcbv8zP",
    "ExecuteTime": {
     "end_time": "2024-06-12T16:57:20.855895Z",
     "start_time": "2024-06-12T16:57:20.852885Z"
    }
   },
   "source": [
    "# don't use these if you are trying this on a local IDE like PyCharm\n",
    "# Installing the libraries with the specified version.\n",
    "# uncomment and run the following lines if Jupyter Notebook is being used\n",
    "# pip install tensorflow==2.13.0 scikit-learn==1.2.2 seaborn==0.11.1 matplotlib==3.3.4 numpy==1.24.3 pandas==1.5.2 opencv-python==4.8.0.76 -q --user\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Installation Notes:\n",
    "I am going to attempt to use PyCharm for the use of this notebook.<br />\n",
    "I understand some of this may take a long time, since I will not be using Google Colabs GPU feature, and everything will be running on CPU only. <br />\n",
    "In Pycharm, I created a new project.  Use the default setups, with the exception of adding a venv environment to keep packages seperate from the rest of a system.<br />\n",
    "If you try to fire up a 'Jupyter' project from PyCharm, not everything gets installed the way you want it, and the project will use universal variables instead of the ones in the venv.<br />\n",
    "The down side of using a local IDE is that each package must be installed seperately using pip.<br />\n",
    "1.  pip install --upgrade pip\n",
    "2. pip install jupyter\n",
    "Once you have these two installed you can pretty much go down the list of the packages listed above, from the 'import' _HOWEVER:_\n",
    "3. cv2 is a package _inside_ of another package and you can't install it directly.  Instead use the follow: <br /> `pip install opencv-python`\n",
    "4. Also don't use pip install sklearn.  it's been deprecated.  Instead use; <br /> `pip install scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ1DoI5YtA18"
   },
   "source": [
    "**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.*"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T22:40:26.383872Z",
     "start_time": "2024-06-12T22:39:46.189481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np                                                                               # Importing numpy for Matrix Operations\n",
    "import pandas as pd                                                                              # Importing pandas to read CSV files\n",
    "import matplotlib.pyplot as plt                                                                  # Importting matplotlib for Plotting and visualizing images\n",
    "import math                                                                                      # Importing math module to perform mathematical operations\n",
    "import cv2                                                                                       # Importing openCV for image processing\n",
    "import seaborn as sns                                                                            # Importing seaborn to plot graphs\n",
    "\n",
    "\n",
    "# Tensorflow modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator                              # Importing the ImageDataGenerator for data augmentation\n",
    "from tensorflow.keras.models import Sequential                                                   # Importing the sequential module to define a sequential model\n",
    "from tensorflow.keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D,BatchNormalization # Defining all the layers to build our CNN Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD                                                 # Importing the optimizers which can be used in our model\n",
    "from sklearn import preprocessing                                                                # Importing the preprocessing module to preprocess the data\n",
    "from sklearn.model_selection import train_test_split                                             # Importing train_test_split function to split the data into train and test\n",
    "from sklearn.metrics import confusion_matrix                                                     # Importing confusion_matrix to plot the confusion matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Display images using OpenCV\n",
    "# from google.colab.patches import cv2_imshow                                                      # Importing cv2_imshow from google.patches to display images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import random\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:40:10.675333: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final Notes:\n",
    "1. Loading the dataset. <br /> you will be loading the dataset locally so don't worry about trying to connect to your Google drive.<br /> I am trying to find a more elegant way to do this using python/jupyter to load from the WWW, but so far I'm still learning just how to set up an environment.\n",
    "2. This project is attached to GitHub.  There is some tricky stuff regarding PyCharm and GitHub, but a topic for another day for sure.  Visual Studio code does this better actually, but I literally hate the look of VS code so I rarely use it unless I am troubleshooting.\n",
    "3. My project repository is at the following location.  I am a bit concerned because I don't think I'll be able to push my images file there.  It's just too big.\n",
    "4. UPDATE: So the npy file is too large, and GitHub rejects the push.  I am installing the 'Git Large File Storage' to try and fix it.  This is not managed in the PyCharm plugins database.  It has to be downloaded and installed from GitHub.  [Here is the url MAC](https://git-lfs.com).<br /> Or use the following universally `brew install git-lfs`. <br /> REFERENCE: [Large File Structure](https://git-lfs.com)\n",
    "5. Then go to your directory managed by venv and git and run the following: `git lfs track \"*.npy\"` this will track the one and only npy package.  Not a big deal, you're not going to update it alot.\n",
    "6. Then run: `git add .gitattributes` to track the attributes of the git.\n",
    "7. UPDATE: Until I figure this out the images.npy will not be added to this repository.  Since nothing will change with this file, contact me for the file and I am happy to send it.\n",
    "8. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Uncomment and run the below code if you are using google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "#load the csv\n",
    "# seedata = pd.read_csv('labels.csv')"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T22:40:33.278612Z",
     "start_time": "2024-06-12T22:40:32.651692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the image file of dataset\n",
    "images = np.load('images.npy')      # Complete the code to read the dataset\n",
    "\n",
    "# Load the labels file of dataset\n",
    "labels = pd.read_csv('labels.csv')  # Complete the code to read the dataset"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the image file of dataset\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m images \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimages.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m      \u001B[38;5;66;03m# Complete the code to read the dataset\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Load the labels file of dataset\u001B[39;00m\n\u001B[1;32m      5\u001B[0m labels \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Complete the code to read the dataset\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pgaiml/seedling_classification_02/.venv/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'images.npy'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# dataset test\n",
    "labels.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Overview"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Understand the shape of the dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(labels.shape)         # Complete the code to check the shape\n",
    "print(images.shape)         # Complete the code to check the shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exploratory Data Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting random images from each of the class"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_images(images,labels):\n",
    "  num_classes=10                                                                  # Number of Classes\n",
    "  categories=np.unique(labels)\n",
    "  keys=dict(labels['Label'])                                                      # Obtaing the unique classes from y_train\n",
    "  rows = 3                                                                        # Defining number of rows=3\n",
    "  cols = 4                                                                        # Defining number of columns=4\n",
    "  fig = plt.figure(figsize=(10, 8))                                               # Defining the figure size to 10x8\n",
    "  for i in range(cols):\n",
    "      for j in range(rows):\n",
    "          random_index = np.random.randint(0, len(labels))                        # Generating random indices from the data and plotting the images\n",
    "          ax = fig.add_subplot(rows, cols, i * rows + j + 1)                      # Adding subplots with 3 rows and 4 columns\n",
    "          ax.imshow(images[random_index, :])                                      # Plotting the image\n",
    "          ax.set_title(keys[random_index])\n",
    "  plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_images(images,labels)   # Complete the code to input the images and labels to the function and plot the images with their labels"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Checking the distribution of the target variable"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.countplot(x=labels['Label'])            # Complete the code to check for data imbalance\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Pre-Processing"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Converting the BGR images to RGB images."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Converting the images from BGR to RGB using cvtColor function of OpenCV\n",
    "for i in range(len(images)):\n",
    "  images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)        # Complete the code to convert the images from BGR to RGB"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Resizing images"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As the size of the images is large, it may be computationally expensive to train on these larger images; therefore, it is preferable to reduce the image size from 128 to 64."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "images_decreased=[]\n",
    "height = ______                    # Complete the code to define the height as 64\n",
    "width =  ______                    # Complete the code to define the width as 64\n",
    "dimensions = (width, height)\n",
    "for i in range(len(images)):\n",
    "  images_decreased.append( cv2.resize(images[i], dimensions, interpolation=cv2.INTER_LINEAR))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Image before resizing**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.imshow(images[3])"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Image after resizing**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.imshow(images_decreased[3])"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation for Modeling\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- As we have less images in our dataset, we will only use 10% of our data for testing, 10% of our data for validation and 80% of our data for training.\n",
    "- We are using the train_test_split() function from scikit-learn. Here, we split the dataset into three parts, train,test and validation."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(np.array(images_decreased),______ , test_size=______, random_state=42,stratify=labels)   # Complete the code to split the data with test_size as 0.1\n",
    "X_train, X_val, y_train, y_val = train_test_split(______,______ , test_size=______, random_state=42,stratify=y_temp)                       # Complete the code to split the data with test_size as 0.1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Complete the code to check the shape of train, validation and test data\n",
    "print(______.shape,y_train.shape)\n",
    "print(X_val.shape,______.shape)\n",
    "print(______.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAJ9B0wKNiY3"
   },
   "source": [
    "### Encoding the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88OIAwNoEPfx"
   },
   "outputs": [],
   "source": [
    "# Convert labels from names to one hot vectors.\n",
    "# We have already used encoding methods like onehotencoder and labelencoder earlier so now we will be using a new encoding method called labelBinarizer.\n",
    "# Labelbinarizer works similar to onehotencoder\n",
    "\n",
    "enc = ______                                        # Complete the code to intialize the labelBinarizer\n",
    "y_train_encoded = enc.fit_transform(______)        # Complete the code to fit and transform y_train\n",
    "y_val_encoded=enc.transform(______)                  # Complete the code to transform y_val\n",
    "y_test_encoded=enc.transform(______)                # Complete the code to transform y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_owIjVzeEMol"
   },
   "outputs": [],
   "source": [
    "y_train_encoded.shape,______.shape,______.shape    # Complete the code to check the shape of train, validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exJFCDSMNrEG"
   },
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wANBL_cOj86C"
   },
   "source": [
    "Since the **image pixel values range from 0-255**, our method of normalization here will be **scaling** - we shall **divide all the pixel values by 255 to standardize the images to have values between 0-1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVUuPJS9OB_U"
   },
   "outputs": [],
   "source": [
    "# Complete the code to normalize the image pixels of train, test and validation data\n",
    "X_train_normalized = ______.astype('float32')/255.0\n",
    "X_val_normalized = X_val.astype('float32')/______\n",
    "X_test_normalized = ______.astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9_M19L-OLng"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze1uI-R1ObD8"
   },
   "outputs": [],
   "source": [
    "# Clearing backend\n",
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIuK82kSOc0Z"
   },
   "outputs": [],
   "source": [
    "# Fixing the seed for random number generators\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpFh02ziOcAy"
   },
   "outputs": [],
   "source": [
    "# Intializing a sequential model\n",
    "model1 = ______                             # Complete the code to intialize a sequential model\n",
    "\n",
    "# Complete the code to add the first conv layer with 128 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n",
    "# Input_shape denotes input image dimension of images\n",
    "model1.add(Conv2D(______, (3, 3), activation='______', padding=\"same\", input_shape=(64, 64, 3)))\n",
    "\n",
    "# Complete the code to add the max pooling to reduce the size of output of first conv layer\n",
    "model1.add(______((2, 2), padding = 'same'))\n",
    "\n",
    "# Complete the code to create two similar convolution and max-pooling layers activation = relu\n",
    "model1.add(Conv2D(64, (3, 3), activation='______', padding=\"same\"))\n",
    "model1.add(______((2, 2), padding = 'same'))\n",
    "\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model1.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "# Complete the code to flatten the output of the conv layer after max pooling to make it ready for creating dense connections\n",
    "model1.add(______())\n",
    "\n",
    "# Complete the code to add a fully connected dense layer with 16 neurons\n",
    "model1.add(Dense(______, activation='relu'))\n",
    "model1.add(Dropout(0.3))\n",
    "# Complete the code to add the output layer with 12 neurons and activation functions as softmax since this is a multi-class classification problem\n",
    "model1.add(Dense(______, activation='softmax'))\n",
    "\n",
    "# Complete the code to use the Adam Optimizer\n",
    "opt=Adam()\n",
    "# Complete the code to Compile the model using suitable metric for loss fucntion\n",
    "model1.compile(optimizer=______, loss='______', metrics=['accuracy'])\n",
    "\n",
    "# Complete the code to generate the summary of the model\n",
    "model1.______()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeh0rJrYO7Cb"
   },
   "source": [
    "<b> Fitting the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSYgFjSfFZrS"
   },
   "outputs": [],
   "source": [
    "# Complete the code to fit the model on train and also using the validation data for validation\n",
    "history_1 = model1.fit(\n",
    "            ______, ______,\n",
    "            epochs=30,\n",
    "            validation_data=(X_val_normalized,y_val_encoded),\n",
    "            batch_size=32,\n",
    "            verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJner8d_WYBy"
   },
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wa8e-zpRWVyZ"
   },
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['accuracy'])\n",
    "plt.plot(history_1.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkAhnIyqWqeM"
   },
   "source": [
    "**Evaluate the model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrdX25VOyhQ-"
   },
   "outputs": [],
   "source": [
    "accuracy = model1.evaluate(______, ______, verbose=2)    # Complete the code to evaluate the model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4fiOuFyxVS3"
   },
   "source": [
    "**Plotting the Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lfy2ylNK7Zi_"
   },
   "outputs": [],
   "source": [
    "# Here we would get the output as probablities for each category\n",
    "y_pred=model1.predict(______)                          # Complete the code to predict the output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbH18RpBdgLc"
   },
   "outputs": [],
   "source": [
    "# Obtaining the categorical values from y_test_encoded and y_pred\n",
    "y_pred_arg=np.argmax(y_pred,axis=1)\n",
    "y_test_arg=np.argmax(y_test_encoded,axis=1)\n",
    "\n",
    "# Plotting the Confusion Matrix using confusion matrix() function which is also predefined in tensorflow module\n",
    "confusion_matrix = tf.math.confusion_matrix(______,______)              # Complete the code to plot the confusion matrix\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.4,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax\n",
    ")\n",
    "# Setting the labels to both the axes\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n",
    "ax.xaxis.set_ticklabels(list(enc.classes_),rotation=40)\n",
    "ax.yaxis.set_ticklabels(list(enc.classes_),rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpiettxXX3X1"
   },
   "source": [
    "**Plotting Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxnM4HJOX2qi"
   },
   "outputs": [],
   "source": [
    "# Plotting the classification report\n",
    "cr=metrics.classification_report(_______,_______)     # Complete the code to plot the classification report\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNKUalx8Jcoi"
   },
   "source": [
    "## Model Performance Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqWaiX1wf41q"
   },
   "source": [
    "**Reducing the Learning Rate:**\n",
    "\n",
    "**ReduceLRonPlateau()** is a function that will be used to decrease the learning rate by some factor, if the loss is not decreasing for some time. This may start decreasing the loss at a smaller learning rate. There is a possibility that the loss may still not decrease. This may lead to executing the learning rate reduction again in an attempt to achieve a lower loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlRncTSBTtj5"
   },
   "outputs": [],
   "source": [
    "# Code to monitor val_accuracy\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU6vqL67bd5a"
   },
   "source": [
    "### **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcyB5b4QTeLz"
   },
   "outputs": [],
   "source": [
    "# Clearing backend\n",
    "from tensorflow.keras import backend\n",
    "backend.clear_session()\n",
    "\n",
    "# Fixing the seed for random number generators\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC2Vb1Vph3Cq"
   },
   "outputs": [],
   "source": [
    "# Complete the code to set the rotation_range to 20\n",
    "train_datagen = ImageDataGenerator(\n",
    "                              rotation_range=20,\n",
    "                              fill_mode='nearest'\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-oolHyJh6Wp"
   },
   "outputs": [],
   "source": [
    "# Intializing a sequential model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Complete the code to add the first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n",
    "# Input_shape denotes input image dimension images\n",
    "model2.add(______(64, (______, ______), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n",
    "\n",
    "# Complete the code to add max pooling to reduce the size of output of first conv layer\n",
    "model2.add(______((2, 2), padding = 'same'))\n",
    "\n",
    "\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model2.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 16 neurons\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "\n",
    "# Complete the code to add dropout with dropout_rate=0.3\n",
    "model2.add(Dropout(______))\n",
    "# Complete the code to add the output layer with 12 neurons and activation functions as softmax since this is a multi-class classification problem\n",
    "model2.add(Dense(12, activation='______'))\n",
    "\n",
    "# Complete the code to initialize Adam Optimimzer\n",
    "opt=______\n",
    "# Complete the code to Compile model\n",
    "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model2.______()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6pmFAu1nFvK"
   },
   "source": [
    "<b> Fitting the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChUJ8eeKYbNj"
   },
   "outputs": [],
   "source": [
    "# Complete the code to fit the model on train data with batch_size=64 and epochs=30\n",
    "# Epochs\n",
    "epochs = ______\n",
    "# Batch size\n",
    "batch_size = ______\n",
    "\n",
    "history = model2.fit(train_datagen.flow(______,y_train_encoded,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False),\n",
    "                                       epochs=epochs,\n",
    "                                       steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n",
    "                                       validation_data=(X_val_normalized,y_val_encoded),\n",
    "                                       verbose=1,callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt5e3T5vnPdn"
   },
   "source": [
    "**Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHiqcKS9zsK1"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc1-VJF6nUqe"
   },
   "source": [
    "**Evaluate the model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQQWGjwhzuQR"
   },
   "outputs": [],
   "source": [
    "accuracy = model2.evaluate(______, y_test_encoded, verbose=2)  # Complete the code to evaluate the model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHmS8i-Wnenu"
   },
   "source": [
    "**Plotting the Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVGeFvZYYP2N"
   },
   "outputs": [],
   "source": [
    "# Complete the code to obtain the output probabilities\n",
    "y_pred=model2.predict(______)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX5vAeYCz7qF"
   },
   "outputs": [],
   "source": [
    "# Obtaining the categorical values from y_test_encoded and y_pred\n",
    "y_pred_arg=np.argmax(y_pred,axis=1)\n",
    "y_test_arg=np.argmax(y_test_encoded,axis=1)\n",
    "\n",
    "# Plotting the Confusion Matrix using confusion matrix() function which is also predefined in tensorflow module\n",
    "confusion_matrix = tf.math.confusion_matrix(______,y_pred_arg)     # Complete the code to obatin the confusion matrix\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.4,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax\n",
    ")\n",
    "# Setting the labels to both the axes\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n",
    "ax.xaxis.set_ticklabels(list(enc.classes_),rotation=40)\n",
    "ax.yaxis.set_ticklabels(list(enc.classes_),rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8rMLLEpnmp2"
   },
   "source": [
    "**Plotting Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPNYbOmTYgI-"
   },
   "outputs": [],
   "source": [
    "# Plotting the classification report\n",
    "cr=metrics.classification_report(_______,_______)     # Complete the code to plot the classification report\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRIwvHcxxl0g"
   },
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-LMkGcExnJd"
   },
   "source": [
    "Comment on the final model you have selected and use the same in the below code to visualize the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvDkLMO7YIdY"
   },
   "source": [
    "### Visualizing the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IMupJqCDlWR"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Visualizing the predicted and correct label of images from test data\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[2])\n",
    "plt.show()\n",
    "## Complete the code to predict the test data using the final model selected\n",
    "print('Predicted Label', enc.inverse_transform(_____.predict((X_test_normalized[2].reshape(1,64,64,3)))))   # reshaping the input image as we are only trying to predict using a single image\n",
    "print('True Label', enc.inverse_transform(y_test_encoded)[2])                                               # using inverse_transform() to get the output label from the output vector\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[33])\n",
    "plt.show()\n",
    "## Complete the code to predict the test data using the final model selected\n",
    "print('Predicted Label', enc.inverse_transform(_______.predict((X_test_normalized[33].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n",
    "print('True Label', enc.inverse_transform(y_test_encoded)[33])                                              # using inverse_transform() to get the output label from the output vector\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[59],)\n",
    "plt.show()\n",
    "## Complete the code to predict the test data using the final model selected\n",
    "print('Predicted Label', enc.inverse_transform(________.predict((X_test_normalized[59].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n",
    "print('True Label', enc.inverse_transform(y_test_encoded)[59])                                              # using inverse_transform() to get the output label from the output vector\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_test[36])\n",
    "plt.show()\n",
    "## Complete the code to predict the test data using the final model selected\n",
    "print('Predicted Label', enc.inverse_transform(______.predict((X_test_normalized[36].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n",
    "print('True Label', enc.inverse_transform(y_test_encoded)[36])                                              # using inverse_transform() to get the output label from the output vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg2x8AyJ4oPR"
   },
   "source": [
    "## Actionable Insights and Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMu_Wnk5yJsO"
   },
   "source": [
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEVFnHXtyMJa"
   },
   "source": [
    "_____"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_Fgd2jg4f0XH",
    "upANL8_-f2gy",
    "ZpUtbBsMf_9O",
    "EPrWNbdYgMQz",
    "qqFzmTb0BKKW",
    "W593kHe2Bfgp",
    "jD8EDTyvg6mD",
    "uE84hQU7CSZa",
    "EYv5uX-MC9KC",
    "ayS8yyFMhy9s",
    "tY8e2flvDup8",
    "o3DMS5DsiVRI",
    "Yylo78cvIteK",
    "NQV0unTvM7XM",
    "FAJ9B0wKNiY3",
    "exJFCDSMNrEG",
    "d9_M19L-OLng",
    "kNKUalx8Jcoi",
    "zU6vqL67bd5a",
    "qRIwvHcxxl0g",
    "dvDkLMO7YIdY",
    "Eg2x8AyJ4oPR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
